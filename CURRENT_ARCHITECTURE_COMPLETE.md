# AetherSignal - Complete Current Architecture Documentation

## ğŸ“‹ Executive Summary

**AetherSignal** is a **server-side Python/Streamlit application** for pharmacovigilance signal detection and analysis. It provides natural language querying, statistical analysis (PRR/ROR), quantum-inspired ranking, and a ChatGPT-like conversational interface for safety data exploration.

**Architecture Type:** Server-Side, Streamlit-Based, Python-First  
**Deployment:** Streamlit Cloud compatible  
**Status:** Fully functional, production-ready  
**Lines of Code:** ~15,000+ lines  
**Language:** Python 3.12  

---

## ğŸ—ï¸ Technology Stack

### **Core Framework:**
- **UI Framework:** Streamlit 1.38.0 (Python-based reactive web framework)
- **Language:** Python 3.12
- **Data Processing:** Pandas 2.2.2, NumPy 1.26.4, SciPy
- **Visualization:** Plotly 5.22.0
- **PDF Generation:** fpdf2 2.8.5

### **Backend Services:**
- **Database:** Supabase (PostgreSQL) with Row-Level Security (RLS)
- **Authentication:** Supabase Auth (email/password, email verification)
- **Storage:** 
  - In-memory (session state) for uploaded files
  - Supabase PostgreSQL for persistent multi-tenant data

### **AI/LLM Integration:**
- **LLM Providers:** OpenAI GPT-4o-mini, Anthropic Claude, Groq (LLaMA-3 70B)
- **Frameworks:** OpenAI SDK, Anthropic SDK, HuggingFace Hub
- **Hybrid Approach:** Rule-based parsing first, LLM fallback optional

### **Additional Libraries:**
- **Fuzzy Matching:** rapidfuzz 3.5.2
- **Quantum Computing:** PennyLane 0.38.0 (quantum-inspired algorithms)
- **ML (Optional):** Transformers 4.35.0, PyTorch (optional)
- **Data Matching:** recordlinkage 0.16.0 (cross-source deduplication)

---

## ğŸ“ Complete File Structure

```
aethersignal/
â”‚
â”œâ”€â”€ app.py                          # Main Streamlit application entry point
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ runtime.txt                     # Python version specification
â”‚
â”œâ”€â”€ pages/                          # Streamlit multi-page app structure
â”‚   â”œâ”€â”€ 1_Quantum_PV_Explorer.py   # Main PV analysis page (protected)
â”‚   â”œâ”€â”€ 2_Social_AE_Explorer.py    # Social media AE analysis (protected)
â”‚   â”œâ”€â”€ Login.py                    # Authentication page
â”‚   â”œâ”€â”€ Register.py                 # User registration page
â”‚   â””â”€â”€ Profile.py                  # User profile management
â”‚
â”œâ”€â”€ src/                            # Core application code
â”‚   â”‚
â”‚   â”œâ”€â”€ ui/                         # User Interface Components
â”‚   â”‚   â”œâ”€â”€ chat_interface.py       # ChatGPT-like chat UI component (NEW)
â”‚   â”‚   â”œâ”€â”€ query_interface.py      # Natural language query interface
â”‚   â”‚   â”œâ”€â”€ upload_section.py       # File upload and processing
â”‚   â”‚   â”œâ”€â”€ results_display.py      # Results visualization (tables, charts)
â”‚   â”‚   â”œâ”€â”€ top_nav.py              # Top navigation bar
â”‚   â”‚   â”œâ”€â”€ sidebar.py              # Sidebar navigation and filters
â”‚   â”‚   â”œâ”€â”€ header.py               # Page header component
â”‚   â”‚   â”œâ”€â”€ schema_mapper.py        # Schema mapping UI
â”‚   â”‚   â”œâ”€â”€ drill_down.py           # Drill-down analysis views
â”‚   â”‚   â”œâ”€â”€ case_series_viewer.py   # Case series viewer
â”‚   â”‚   â””â”€â”€ auth/                   # Auth UI components
â”‚   â”‚       â”œâ”€â”€ login.py
â”‚   â”‚       â”œâ”€â”€ register.py
â”‚   â”‚       â””â”€â”€ profile.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ai/                         # AI/LLM Components
â”‚   â”‚   â”œâ”€â”€ hybrid_router.py        # Route queries (rule-based + LLM)
â”‚   â”‚   â”œâ”€â”€ conversational_engine.py # Process conversational queries
â”‚   â”‚   â”œâ”€â”€ llm_interpreter.py      # LLM query interpretation
â”‚   â”‚   â”œâ”€â”€ medical_llm.py          # Unified LLM interface
â”‚   â”‚   â”œâ”€â”€ signal_summarizer.py    # Generate signal summaries
â”‚   â”‚   â”œâ”€â”€ narrative_analyzer.py   # Analyze case narratives
â”‚   â”‚   â”œâ”€â”€ literature_enhancer.py  # Literature integration
â”‚   â”‚   â”œâ”€â”€ meddra_enhancer.py      # MedDRA terminology enhancement
â”‚   â”‚   â””â”€â”€ stream_helpers.py       # Streaming UI helpers (NEW)
â”‚   â”‚
â”‚   â”œâ”€â”€ auth/                       # Authentication & User Management
â”‚   â”‚   â”œâ”€â”€ auth.py                 # Core auth functions (login, register, logout)
â”‚   â”‚   â””â”€â”€ user_management.py      # User profile management
â”‚   â”‚
â”‚   â”œâ”€â”€ social_ae/                  # Social Media Adverse Event Module
â”‚   â”‚   â”œâ”€â”€ social_fetcher.py       # Fetch from Reddit/X
â”‚   â”‚   â”œâ”€â”€ social_cleaner.py       # Clean and normalize
â”‚   â”‚   â”œâ”€â”€ social_mapper.py        # Map to PV schema
â”‚   â”‚   â”œâ”€â”€ ml_classifier.py        # ML-based AE classification
â”‚   â”‚   â”œâ”€â”€ social_anonymizer.py    # PII anonymization
â”‚   â”‚   â”œâ”€â”€ social_storage.py       # Storage utilities
â”‚   â”‚   â”œâ”€â”€ social_ae_integration.py # Integration layer
â”‚   â”‚   â”œâ”€â”€ social_dashboard.py     # Dashboard UI
â”‚   â”‚   â””â”€â”€ social_ae_scheduler.py  # Scheduled fetching
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                       # Core Processing Modules
â”‚   â”‚   â”œâ”€â”€ signal_stats.py         # PRR/ROR, statistical analysis
â”‚   â”‚   â”œâ”€â”€ nl_query_parser.py      # Natural language query parsing
â”‚   â”‚   â”œâ”€â”€ query_correction.py     # Query typo correction
â”‚   â”‚   â”œâ”€â”€ faers_loader.py         # FAERS file format loader
â”‚   â”‚   â”œâ”€â”€ pv_schema.py            # PV schema detection
â”‚   â”‚   â”œâ”€â”€ pv_storage.py           # Database storage/retrieval
â”‚   â”‚   â”œâ”€â”€ utils.py                # Utility functions
â”‚   â”‚   â”œâ”€â”€ case_processing.py      # Case processing logic
â”‚   â”‚   â””â”€â”€ app_helpers.py          # Application helpers
â”‚   â”‚
â”‚   â”œâ”€â”€ advanced/                   # Advanced Analysis Features
â”‚   â”‚   â”œâ”€â”€ quantum_ranking.py      # Quantum-inspired ranking
â”‚   â”‚   â”œâ”€â”€ quantum_clustering.py   # Quantum clustering
â”‚   â”‚   â”œâ”€â”€ quantum_anomaly.py      # Anomaly detection
â”‚   â”‚   â”œâ”€â”€ quantum_duplicate_detection.py # Duplicate detection
â”‚   â”‚   â”œâ”€â”€ quantum_explainability.py # Explainability
â”‚   â”‚   â”œâ”€â”€ longitudinal_spike.py   # Temporal spike detection
â”‚   â”‚   â”œâ”€â”€ time_to_onset.py        # Time-to-onset analysis
â”‚   â”‚   â”œâ”€â”€ class_effect_detection.py # Drug class effects
â”‚   â”‚   â”œâ”€â”€ subgroup_discovery.py   # Subgroup analysis
â”‚   â”‚   â”œâ”€â”€ signal_prioritization.py # Signal prioritization
â”‚   â”‚   â”œâ”€â”€ new_signal_detection.py # New signal detection
â”‚   â”‚   â”œâ”€â”€ advanced_stats.py       # Advanced statistics
â”‚   â”‚   â””â”€â”€ cross_source_deduplication.py # Cross-source dedup
â”‚   â”‚
â”‚   â”œâ”€â”€ normalization/              # Data Normalization
â”‚   â”‚   â”œâ”€â”€ drug_name_normalization.py # Drug name standardization
â”‚   â”‚   â”œâ”€â”€ exposure_normalization.py  # Exposure normalization
â”‚   â”‚   â””â”€â”€ mapping_templates.py      # Mapping templates
â”‚   â”‚
â”‚   â”œâ”€â”€ integration/                # External Integrations
â”‚   â”‚   â”œâ”€â”€ literature_integration.py # Literature APIs
â”‚   â”‚   â”œâ”€â”€ e2b_import.py           # E2B format import
â”‚   â”‚   â”œâ”€â”€ e2b_export.py           # E2B format export
â”‚   â”‚   â””â”€â”€ analytics.py            # Analytics tracking
â”‚   â”‚
â”‚   â”œâ”€â”€ reporting/                  # Reporting & Export
â”‚   â”‚   â”œâ”€â”€ pdf_report.py           # PDF report generation
â”‚   â”‚   â””â”€â”€ audit_trail.py          # Audit logging
â”‚   â”‚
â”‚   â”œâ”€â”€ watchlist_tab.py            # Drug watchlist feature
â”‚   â”œâ”€â”€ styles.py                   # Global CSS styles
â”‚   â””â”€â”€ llm_explain.py              # LLM explanation utilities
â”‚
â”œâ”€â”€ database/                       # Database Schema
â”‚   â”œâ”€â”€ schema.sql                  # Main schema (user_profiles, pv_cases)
â”‚   â””â”€â”€ schema_tenant_upgrade.sql   # Multi-tenant upgrades
â”‚
â”œâ”€â”€ api/                            # API Endpoints (if any)
â”‚   â””â”€â”€ social_api.py               # Social AE API
â”‚
â”œâ”€â”€ analytics/                      # Analytics Data
â”‚   â””â”€â”€ audit_log.jsonl             # Audit trail logs
â”‚
â””â”€â”€ .env                            # Environment variables (not in git)
```

---

## ğŸ”„ Complete Data Flow

### **1. Application Startup Flow:**

```
app.py (Entry Point)
    â†“
Load .env file (dotenv)
    â†“
Restore authentication session (restore_session())
    â†“
Apply theme (apply_theme())
    â†“
Render top navigation (render_top_nav())
    â†“
User selects page (Streamlit navigation)
```

### **2. Authentication Flow:**

```
User visits Login/Register page
    â†“
Enter credentials
    â†“
Supabase Auth API call
    â†“
Email verification (if new user)
    â†“
Create/Update user profile (user_profiles table)
    â†“
Store session token (st.session_state.user_session)
    â†“
Redirect to protected page
```

### **3. Data Upload & Processing Flow:**

```
User uploads file (upload_section.py)
    â†“
Detect file type (CSV, Excel, ZIP, FAERS)
    â†“
Parse file (faers_loader.py or pandas)
    â†“
Schema detection (pv_schema.py)
    â†“
Column mapping (schema_mapper.py UI)
    â†“
Data normalization (drug_name_normalization.py, etc.)
    â†“
Store in session state (st.session_state.normalized_data)
    â†“
IF authenticated:
    â†“
    Store in Supabase (pv_storage.py)
    â†“
    Batch insert with user_id + organization
    â†“
    RLS automatically filters by user/company
```

### **4. Query Processing Flow (Current Implementation):**

```
User enters query (chat_interface.py or query_interface.py)
    â†“
Query correction (query_correction.py) - optional
    â†“
Hybrid Router (hybrid_router.py):
    â”œâ”€ Rule-based parser (nl_query_parser.py) â† Tries first
    â””â”€ LLM interpreter (llm_interpreter.py) â† Fallback if enabled
    â†“
Extract filters (drug, reaction, age, date, etc.)
    â†“
Load data:
    â”œâ”€ IF in session: Use st.session_state.normalized_data
    â””â”€ IF authenticated: Load from Supabase (pv_storage.py)
    â†“
Apply filters (signal_stats.apply_filters())
    â†“
Calculate statistics:
    â”œâ”€ Summary stats (get_summary_stats())
    â”œâ”€ PRR/ROR (calculate_prr_ror())
    â”œâ”€ Trends (longitudinal_spike.py)
    â””â”€ Demographics (age, sex distribution)
    â†“
Generate response:
    â”œâ”€ Rule-based summary (conversational_engine.py)
    â””â”€ LLM summary (signal_summarizer.py) - if enabled
    â†“
Display results (results_display.py):
    â”œâ”€ Chat interface (chat_interface.py) â† Shows AI response
    â”œâ”€ Overview tab (metrics, KPIs)
    â”œâ”€ Signals tab (PRR/ROR, charts)
    â”œâ”€ Trends tab (time series, spikes)
    â”œâ”€ Cases tab (data table)
    â””â”€ Report tab (PDF download)
```

### **5. Database Interaction Flow:**

```
Supabase Connection (pv_storage.py)
    â†“
Authenticate with service_role key (for writes)
    OR anon key with user session (for reads)
    â†“
RLS Policies (database/schema.sql):
    â”œâ”€ Users can only see their organization's data
    â”œâ”€ Automatic filtering by user_id
    â””â”€ Enforced at database level
    â†“
Query/Insert Operations:
    â”œâ”€ store_pv_data() â†’ Batch insert with user_id + org
    â”œâ”€ load_pv_data() â†’ SELECT with user_id filter
    â””â”€ get_user_data_stats() â†’ Aggregated stats per user
```

---

## ğŸ§© Component Architecture

### **Layer 1: UI Layer (Streamlit Components)**

**Purpose:** User interface rendering and interaction

**Key Files:**
- `app.py` - Application entry point
- `pages/*.py` - Multi-page navigation
- `src/ui/*.py` - Reusable UI components

**Responsibilities:**
- Render Streamlit UI elements
- Handle user input
- Manage session state (`st.session_state`)
- Display results and charts

**State Management:**
```python
# All state managed via Streamlit session state
st.session_state.normalized_data    # Current dataset
st.session_state.chat_history       # Chat conversation history
st.session_state.last_filters       # Last query filters
st.session_state.last_query_text    # Last query text
st.session_state.show_results       # Show results flag
st.session_state.user_session       # Supabase auth session
```

### **Layer 2: Business Logic Layer**

**Purpose:** Core application logic and data processing

**Key Modules:**

#### **A. Query Processing (`nl_query_parser.py`, `query_correction.py`)**
- Parse natural language queries
- Extract filters (drug, reaction, age, date, etc.)
- Typo correction and query suggestions
- Intent detection

#### **B. Statistical Analysis (`signal_stats.py`)**
- Filter data by criteria
- Calculate PRR/ROR with confidence intervals
- Summary statistics (counts, percentages)
- Demographic analysis
- Time trend analysis

#### **C. Data Processing (`faers_loader.py`, `case_processing.py`)**
- File parsing (FAERS, CSV, Excel)
- Data normalization
- Schema mapping
- Data cleaning

#### **D. Advanced Analytics (`quantum_*.py`, `longitudinal_spike.py`)**
- Quantum-inspired ranking
- Temporal spike detection
- Subgroup discovery
- Class effect detection

### **Layer 3: AI/LLM Layer**

**Purpose:** Natural language understanding and generation

**Key Components:**

#### **A. Hybrid Router (`hybrid_router.py`)**
```python
# Routes queries through rule-based first, LLM fallback
filters, method, confidence = route_query(
    query, normalized_df, use_llm=False
)
```

**Strategy:**
1. Try rule-based parser first (fast, deterministic)
2. If confidence < threshold AND use_llm=True â†’ Try LLM
3. Return filters with method and confidence score

#### **B. Conversational Engine (`conversational_engine.py`)**
```python
# Processes query and generates response
result = process_conversational_query(
    query, normalized_df, use_llm=False
)
```

**Flow:**
1. Route query â†’ Extract filters
2. Apply filters â†’ Get filtered dataset
3. Calculate statistics
4. Generate natural language response (rule-based or LLM)

#### **C. LLM Integration (`medical_llm.py`, `llm_interpreter.py`)**
- Unified interface for multiple LLM providers (OpenAI, Claude, Groq)
- Query interpretation
- Response generation
- Error handling and fallbacks

### **Layer 4: Data Layer**

**Purpose:** Data persistence and retrieval

#### **A. Session Storage (In-Memory)**
- `st.session_state` - Temporary session data
- Cleared on page refresh (unless persisted)

#### **B. Database Storage (Supabase PostgreSQL)**
- **Tables:**
  - `user_profiles` - User accounts and organizations
  - `pv_cases` - Pharmacovigilance case data (multi-tenant)
  
- **Features:**
  - Row-Level Security (RLS) for multi-tenancy
  - Automatic data isolation by organization
  - Batch inserts for performance
  - User-scoped queries

#### **C. Storage Functions (`pv_storage.py`)**
```python
# Store data with user/company association
store_pv_data(df, user_id, organization, source)

# Load data filtered by user/company (RLS enforced)
load_pv_data(user_id, organization)

# Get statistics
get_user_data_stats(user_id, organization)
```

### **Layer 5: Integration Layer**

**Purpose:** External service integration

- **Supabase Auth** - Authentication
- **Supabase Database** - Data persistence
- **LLM APIs** - OpenAI, Anthropic, Groq
- **Social Media APIs** - Reddit, X (Twitter) - for Social AE module

---

## ğŸ” Authentication & Authorization

### **Authentication Flow:**

1. **Registration:**
   ```
   User fills form â†’ Supabase Auth API â†’ Email verification
   â†’ Create user_profiles record â†’ Login
   ```

2. **Login:**
   ```
   User credentials â†’ Supabase Auth API â†’ Get session token
   â†’ Store in st.session_state.user_session â†’ Restore on page load
   ```

3. **Session Management:**
   - Session token stored in `st.session_state.user_session`
   - `restore_session()` called on every page load
   - Session persists across Streamlit page navigation

### **Authorization (Multi-Tenant):**

**Database-Level (RLS):**
```sql
-- Users can only see their organization's data
CREATE POLICY "Users can view own company data"
    ON pv_cases FOR SELECT
    USING (
        auth.uid() = user_id OR
        EXISTS (
            SELECT 1 FROM user_profiles
            WHERE user_profiles.id = auth.uid()
            AND user_profiles.organization = pv_cases.organization
        )
    );
```

**Application-Level:**
- Protected pages check `is_authenticated()` before rendering
- Database queries automatically filtered by RLS
- UI shows/hides features based on auth state

---

## ğŸ“Š Key Features & Capabilities

### **1. Natural Language Querying**
- Plain English queries ("Show me Dupixent conjunctivitis cases")
- Query correction and suggestions
- Hybrid parsing (rule-based + LLM)
- Multi-turn conversation support (chat interface)

### **2. Statistical Analysis**
- PRR/ROR with 95% confidence intervals
- Disproportionality analysis
- Time trend detection
- Spike detection
- Demographic analysis

### **3. ChatGPT-Like Interface**
- Conversational chat interface
- Streaming responses
- Progressive updates
- Multi-turn context
- Natural language responses

### **4. Data Management**
- Multi-format file support (FAERS, CSV, Excel, ZIP)
- Automatic schema detection
- Column mapping UI
- Data normalization
- Persistent storage (Supabase)

### **5. Advanced Features**
- Quantum-inspired ranking
- Signal prioritization
- Cross-source deduplication
- Class effect detection
- Subgroup discovery

### **6. Reporting**
- PDF report generation
- Exportable results
- Audit trail logging

---

## ğŸ”„ Request/Response Flow Example

### **Example: User Query "Show me Dupixent conjunctivitis cases"**

```
1. User enters query in chat interface
   â†“
2. Chat interface calls on_send("Show me Dupixent conjunctivitis cases")
   â†“
3. Add user message to chat_history
   â†“
4. Show "thinking" indicator
   â†“
5. Query correction (if enabled):
   - Check for typos
   - Suggest corrections if needed
   â†“
6. Hybrid Router:
   - Rule-based parser extracts: drug="Dupixent", reaction="conjunctivitis"
   - Returns filters + confidence score
   â†“
7. Load data:
   - Check session state first
   - If authenticated: Load from Supabase with RLS filtering
   â†“
8. Apply filters:
   - Filter DataFrame: drug_name contains "Dupixent"
   - Filter DataFrame: reaction contains "conjunctivitis"
   â†“
9. Calculate statistics:
   - Matching cases: 1,234
   - Total cases: 438,512
   - Percentage: 0.28%
   - PRR/ROR (if drug+reaction both specified)
   - Time trends
   - Demographics
   â†“
10. Generate response:
    - Rule-based: Format statistics into natural language
    - OR LLM: Pass stats to LLM for natural language summary
    â†“
11. Update chat interface:
    - Replace "thinking" with final response
    - Store metadata (filters, stats) in message
    â†“
12. Display results:
    - Chat shows AI response
    - Results tabs show:
      - Overview: Metrics, KPIs
      - Signals: PRR/ROR charts
      - Trends: Time series
      - Cases: Data table
      - Report: PDF download
```

---

## ğŸ¨ UI Architecture

### **Streamlit Multi-Page App:**

```
app.py (Landing Page)
â”œâ”€â”€ pages/1_Quantum_PV_Explorer.py (Main PV Analysis)
â”‚   â”œâ”€â”€ Sidebar: Filters, upload, settings
â”‚   â”œâ”€â”€ Main Area: Tabs
â”‚   â”‚   â”œâ”€â”€ Upload Tab: File upload + schema mapping
â”‚   â”‚   â”œâ”€â”€ Natural Language Query Tab: Chat interface + query input
â”‚   â”‚   â”œâ”€â”€ Advanced Search Tab: Structured filters
â”‚   â”‚   â””â”€â”€ Watchlist Tab: Drug watchlist
â”‚   â””â”€â”€ Results: Displayed below (when show_results=True)
â”‚
â”œâ”€â”€ pages/2_Social_AE_Explorer.py (Social Media AE)
â”‚   â””â”€â”€ Social media adverse event analysis
â”‚
â”œâ”€â”€ pages/Login.py (Authentication)
â”œâ”€â”€ pages/Register.py (Registration)
â””â”€â”€ pages/Profile.py (User Profile)
```

### **Component Hierarchy:**

```
Top Navigation (top_nav.py)
    â†“
Page Content (pages/*.py)
    â”œâ”€â”€ Sidebar (sidebar.py)
    â”‚   â”œâ”€â”€ Filters
    â”‚   â”œâ”€â”€ Upload Section
    â”‚   â””â”€â”€ Settings
    â”‚
    â””â”€â”€ Main Content
        â”œâ”€â”€ Upload Section (upload_section.py)
        â”œâ”€â”€ Query Interface (query_interface.py)
        â”‚   â””â”€â”€ Chat Interface (chat_interface.py) â† NEW
        â”œâ”€â”€ Results Display (results_display.py)
        â””â”€â”€ Other Components
```

---

## ğŸ—„ï¸ Database Schema

### **user_profiles Table:**
```sql
- id (UUID, PK, FK to auth.users)
- email (text)
- organization (text)
- created_at (timestamp)
- updated_at (timestamp)
```

### **pv_cases Table:**
```sql
- id (UUID, PK)
- user_id (UUID, FK to auth.users)
- organization (text)
- drug_name (text)
- reaction (text)
- age (numeric)
- sex (text)
- country (text)
- serious (boolean)
- outcome (text)
- report_date (date)
- ... (other PV fields)
- raw_data (jsonb) - Original row data
- created_at (timestamp)
- updated_at (timestamp)
```

### **RLS Policies:**
- Users can only SELECT their organization's data
- Users can only INSERT with their user_id
- Users can only UPDATE/DELETE their own records

---

## ğŸ”Œ External Dependencies

### **APIs:**
- **Supabase Auth API** - Authentication
- **Supabase Database API** - Data persistence (PostgreSQL)
- **OpenAI API** - GPT-4o-mini
- **Anthropic API** - Claude
- **Groq API** - LLaMA-3 70B

### **Libraries:**
- **Streamlit** - UI framework
- **Pandas** - Data processing
- **NumPy/SciPy** - Statistical computing
- **Plotly** - Visualization
- **Supabase Python SDK** - Database client
- **rapidfuzz** - Fuzzy matching
- **PennyLane** - Quantum computing

---

## ğŸš€ Deployment Architecture

### **Current Deployment:**
- **Platform:** Streamlit Cloud (recommended)
- **Requirements:**
  - Python 3.12
  - Dependencies from requirements.txt
  - Environment variables in Streamlit Cloud dashboard

### **Alternative Deployments:**
- **Docker:** Dockerfile provided
- **Railway:** railway.json provided
- **Render:** render.yaml provided

### **Environment Variables:**
```env
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_ANON_KEY=xxx
SUPABASE_SERVICE_KEY=xxx
SUPABASE_DB_PASSWORD=xxx
OPENAI_API_KEY=xxx (optional)
ANTHROPIC_API_KEY=xxx (optional)
GROQ_API_KEY=xxx (optional)
```

---

## ğŸ“ˆ Performance Characteristics

### **Current Performance:**
- **File Upload:** 1-5 minutes for 100MB FAERS file
- **Query Processing:** 200-2000ms (rule-based) or 2-5s (with LLM)
- **Database Storage:** Batch inserts, ~1000 rows/second
- **Large Datasets:** Handles 500K+ rows in memory

### **Bottlenecks:**
- File parsing (pandas read operations)
- Large DataFrame operations
- Database batch inserts
- LLM API calls (if enabled)

### **Optimizations Applied:**
- Caching for unique values (drug/reaction lists)
- Batch database inserts
- Query correction caching
- Lazy loading of data

---

## ğŸ”’ Security Architecture

### **Authentication:**
- Supabase Auth (industry standard)
- Email verification required
- Password hashing (handled by Supabase)

### **Authorization:**
- Row-Level Security (RLS) at database level
- Multi-tenant data isolation
- User-scoped queries

### **Data Privacy:**
- User data isolated by organization
- RLS policies prevent cross-tenant access
- Audit trail logging

---

## ğŸ“ Key Design Decisions

### **1. Why Streamlit?**
- Rapid development
- Python-first (matches data science stack)
- Built-in components (tables, charts, file upload)
- Easy deployment (Streamlit Cloud)

### **2. Why Server-Side Processing?**
- No browser memory limitations
- Full pandas/NumPy/SciPy ecosystem
- Can handle large datasets (500K+ rows)
- Mature libraries and tools

### **3. Why Hybrid Router?**
- Rule-based is fast and deterministic
- LLM adds flexibility but is slower/costly
- User can choose based on needs
- Privacy option (rule-based only)

### **4. Why Supabase?**
- PostgreSQL (powerful, familiar)
- Built-in auth
- Row-Level Security (multi-tenant)
- Managed service (no infrastructure)

### **5. Why Session State + Database?**
- Session state for temporary/uploads
- Database for persistent/user data
- Allows both use cases (anonymous + authenticated)

---

## ğŸ¯ Current Status

### **âœ… Completed Features:**
- âœ… Full authentication system
- âœ… Multi-tenant database with RLS
- âœ… Natural language querying
- âœ… ChatGPT-like conversational interface
- âœ… Statistical analysis (PRR/ROR, trends)
- âœ… File upload and processing
- âœ… Results visualization
- âœ… PDF report generation
- âœ… Advanced analytics (quantum ranking, spike detection)
- âœ… Social media AE integration
- âœ… Query correction and suggestions

### **âš ï¸ Known Limitations:**
- File upload can be slow for very large files (>100MB)
- LLM features require API keys
- Session state lost on page refresh (unless using database)
- Browser must stay open during processing

### **ğŸš€ Ready for:**
- Production deployment
- User testing
- Feature enhancements
- Performance optimization

---

## ğŸ“š Additional Documentation

For more details, see:
- `PROJECT_SUMMARY_FOR_AI_REVIEW.md` - Project overview
- `CHATGPT_INTERFACE_ARCHITECTURE_EXPLANATION.md` - Chat interface details
- `ARCHITECTURE_SHIFT_ANALYSIS.md` - Analysis of alternative architectures
- `CHAT_INTERFACE_IMPLEMENTATION_COMPLETE.md` - Chat implementation details

---

## ğŸ”„ Summary: How Everything Connects

```
User Browser
    â†“
Streamlit Server (Python)
    â”œâ”€ UI Layer (Streamlit components)
    â”œâ”€ Business Logic (pandas, NumPy, SciPy)
    â”œâ”€ AI Layer (LLM APIs)
    â””â”€ Data Layer (Supabase)
        â†“
    PostgreSQL Database (RLS enforced)
```

**All processing happens server-side.**
**All state managed via Streamlit session state + Supabase database.**
**All UI rendered server-side via Streamlit's reactive framework.**

---

**Document Version:** 1.0  
**Last Updated:** November 2025  
**Status:** Current as of chat interface implementation completion

