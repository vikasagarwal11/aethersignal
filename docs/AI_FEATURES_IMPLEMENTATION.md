# AI Features Implementation - Option 3 (Hybrid with Privacy Controls)

## Overview

AetherSignal now includes optional AI/LLM-powered features for enhanced query interpretation and signal summarization. All AI features are **opt-in only** with strict privacy controls.

## Architecture

### 1. Hybrid Query Router (`src/ai/hybrid_router.py`)
- **Primary**: Rule-based parser (always runs first)
- **Fallback**: LLM interpreter (only if confidence < threshold AND user opted in)
- **Confidence scoring**: Calculates parsing confidence (0-1)
- **Privacy**: LLM only used when explicitly enabled

### 2. LLM Interpreter (`src/ai/llm_interpreter.py`)
- Supports OpenAI (GPT-4o-mini) and Groq (LLaMA-3 70B)
- Converts natural language to structured filters
- Uses dataset context for accurate drug/reaction matching
- Gracefully degrades if API unavailable

### 3. Conversational Safety Engine (`src/ai/conversational_engine.py`)
- Processes conversational queries
- Generates natural language responses
- Detects red flags and trends
- Combines PRR/ROR, trends, demographics

### 4. Signal Summarizer (`src/ai/signal_summarizer.py`)
- Comprehensive signal summaries
- Conversational responses
- Builds on existing `llm_explain.py`
- Rule-based fallback if LLM unavailable

## Features

### âœ… What's Included

1. **Hybrid Query Parsing**
   - Rule-based parser runs first (fast, private)
   - LLM fallback only if confidence < 0.6 AND user opted in
   - Confidence score displayed to user

2. **Conversational Responses**
   - Natural language answers to queries
   - Red flag detection
   - Trend interpretation
   - Demographics summary

3. **Enhanced Signal Summaries**
   - Comprehensive summaries with interpretation
   - Medical context and risk assessment
   - Trend analysis included

4. **Privacy Controls**
   - Explicit opt-in checkbox
   - Clear privacy warnings
   - Data never leaves system unless user enables LLM
   - Rule-based always available as fallback

## Usage

### For Users

1. **Enable AI Features** (optional):
   - Check "ðŸ¤– Enable AI-enhanced query interpretation" in query interface
   - Read and accept privacy notice
   - Query will use LLM if rule-based confidence is low

2. **View Conversational Responses**:
   - If LLM enabled, "ðŸ’¬ Conversational" tab appears in results
   - Shows natural language interpretation
   - Includes red flags and trends

3. **Generate Comprehensive Summaries**:
   - Click "ðŸ“Š Generate Comprehensive Signal Summary" in conversational tab
   - LLM generates detailed medical interpretation

### For Developers

#### Environment Variables

```bash
# Optional - only needed if using LLM features
OPENAI_API_KEY=your_key_here
GROQ_API_KEY=your_key_here  # Alternative to OpenAI
```

#### Integration Points

1. **Query Interface** (`src/ui/query_interface.py`):
   - Uses `hybrid_router.route_query()` instead of direct parser
   - Shows opt-in checkbox with privacy warning

2. **Results Display** (`src/ui/results_display.py`):
   - Shows query method and confidence
   - Adds conversational tab if LLM enabled
   - Integrates signal summarization

## Privacy & Security

### âœ… Privacy-First Design

- **Default**: All queries use rule-based parser (100% private)
- **Opt-in**: User must explicitly enable LLM features
- **Transparency**: Clear warnings about data leaving system
- **Fallback**: System works perfectly without LLM

### âš ï¸ When LLM is Enabled

- Query text is sent to external API (OpenAI/Groq)
- No patient data or case details are sent
- Only the query text itself is processed
- Results are generated and displayed locally

## Configuration

### Option 3: Hybrid (Current Implementation)

- **Primary**: Rule-based parser
- **Fallback**: LLM (OpenAI GPT-4o-mini or Groq LLaMA-3)
- **Confidence Threshold**: 0.6 (configurable)
- **Privacy**: Opt-in only

### Alternative Configurations

To switch to different providers, modify `src/ai/llm_interpreter.py`:

```python
# Option A: OpenAI Priority
provider = "openai"  # GPT-4o-mini

# Option B: Groq Priority  
provider = "groq"  # LLaMA-3 70B

# Option C: Balanced (already implemented)
# Tries specified provider, falls back to other
```

## Files Created

1. `src/ai/__init__.py` - Module initialization
2. `src/ai/hybrid_router.py` - Query routing logic
3. `src/ai/llm_interpreter.py` - LLM query interpretation
4. `src/ai/conversational_engine.py` - Conversational responses
5. `src/ai/signal_summarizer.py` - Enhanced signal summaries

## Files Modified

1. `src/ui/query_interface.py` - Added opt-in checkbox and hybrid router
2. `src/ui/results_display.py` - Added conversational tab and method display

## Testing

### Without LLM (Default)
- All features work with rule-based parser
- No API keys needed
- 100% private

### With LLM (Opt-in)
1. Set `OPENAI_API_KEY` or `GROQ_API_KEY` environment variable
2. Enable checkbox in query interface
3. Test with conversational queries like:
   - "Is Dupixent showing any new red flags recently?"
   - "Are there signals for pancreatitis with GLP-1 agonists?"

## Limitations

1. **LLM Dependency**: Requires internet and API keys
2. **Cost**: ~$0.01-0.05 per query (OpenAI GPT-4o-mini)
3. **Latency**: 1-3 seconds for LLM responses
4. **Privacy**: Query text sent externally when enabled

## Future Enhancements

- [ ] Local LLM support (Ollama, etc.)
- [ ] Caching of LLM responses
- [ ] Batch processing for multiple queries
- [ ] Custom prompt templates
- [ ] Multi-language support

## Summary

âœ… **Production-ready hybrid system**
âœ… **Privacy-first with opt-in controls**
âœ… **Works perfectly without LLM**
âœ… **Enhanced capabilities when LLM enabled**
âœ… **Regulatory-friendly (deterministic primary, optional AI)**

