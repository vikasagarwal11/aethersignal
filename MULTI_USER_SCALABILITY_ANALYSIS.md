# Multi-User Scalability Analysis: Current Architecture

## üéØ Your Question

**"How can we handle the scenario where multiple users will upload multiple files with heavy data?"**

---

## ‚ö†Ô∏è Current Architecture Limitations

### **Current Design: Server-Side Streamlit**

```
User 1 Browser ‚îÄ‚îÄ‚îê
User 2 Browser ‚îÄ‚îÄ‚î§
User 3 Browser ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚Üí Single Streamlit Server Instance
User 4 Browser ‚îÄ‚îÄ‚î§     (Python process with limited resources)
User N Browser ‚îÄ‚îÄ‚îò
```

### **Critical Bottlenecks:**

#### **1. Memory Constraints (Single Server Process)**

**Current Behavior:**
- Each user session loads data into server RAM
- Each large file (100MB) = ~500K-1M rows in memory
- Pandas DataFrames consume significant memory

**Example Scenario:**
```
10 concurrent users √ó 100MB files = 1GB RAM just for data
+ Python overhead = ~2-3GB total RAM needed
+ Processing overhead = Additional 1-2GB
= ~4-5GB RAM required for 10 concurrent users
```

**Streamlit Cloud Limits:**
- Free tier: Limited memory (varies)
- Paid tier: Still single process (may scale vertically, but expensive)

#### **2. CPU Constraints (Single-Threaded Processing)**

**Current Behavior:**
- File processing is CPU-intensive (pandas operations)
- Each file upload blocks processing for that user
- Multiple uploads = sequential processing (queue)

**Problem:**
```
User 1 uploads 100MB file ‚Üí Processes for 5 minutes
User 2 uploads 100MB file ‚Üí Waits for User 1, then processes for 5 minutes
User 3 uploads 100MB file ‚Üí Waits for User 1 & 2, then processes...

Total time for 3 users: 15 minutes
```

#### **3. Database Connection Limits**

**Current Implementation:**
- Each upload opens Supabase connection
- Batch inserts are blocking (synchronous)
- 100K rows = 2-5 minutes of database insertion

**Supabase Limits:**
- Connection pool limits (varies by plan)
- Rate limiting on inserts
- RLS policy checks on every insert

#### **4. No Background Processing**

**Current Behavior:**
- All processing happens synchronously
- UI blocks during upload/processing
- User must keep browser open

**Impact:**
- If user closes browser, upload is lost
- No queue system for large uploads
- No progress persistence

---

## üî¥ Real-World Scenarios & What Happens

### **Scenario 1: 5 Users Upload 50MB Files Simultaneously**

**What Happens:**
```
Time 0:00 - All 5 users click "Upload"
Time 0:01 - User 1's file starts processing
Time 1:00 - User 1's file finishes, User 2 starts
Time 2:00 - User 2's file finishes, User 3 starts
Time 3:00 - User 3's file finishes, User 4 starts
Time 4:00 - User 4's file finishes, User 5 starts
Time 5:00 - User 5's file finishes

Result: Sequential processing, not concurrent
Last user waits 5 minutes before processing even starts
```

**Memory Usage:**
- Peak: ~1-2GB RAM (only 1 user processing at a time)
- Database: 5 connections in use sequentially

**User Experience:**
- ‚ùå Users 2-5 see "waiting" for minutes
- ‚ùå No visibility into queue position
- ‚ùå Timeout risk if processing takes too long

---

### **Scenario 2: 10 Users Upload 100MB Files**

**What Happens:**
```
Server RAM: ~4-5GB needed
Processing Time: ~50 minutes total (sequential)
Database: Potential connection pool exhaustion
RLS Overhead: Each insert checks RLS policy

Result: 
- Server may crash (out of memory)
- Database may throttle/block connections
- Users experience timeouts
- Poor user experience
```

---

### **Scenario 3: 1 User Uploads 500MB File (Very Large)**

**What Happens:**
```
File Size: 500MB
Rows: ~2.5M rows
Memory: ~2-3GB RAM needed
Processing: ~15-20 minutes
Database: ~10-15 minutes for batch inserts

Result:
- Single user blocks server for 30+ minutes
- Other users cannot upload during this time
- High risk of timeout/crash
```

---

## ‚úÖ Solutions Within Current Architecture

### **Solution 1: Optimize Database Storage (Immediate)**
**Current Code:** `src/pv_storage.py`

**Changes:**
```python
# Current: Synchronous blocking inserts
with st.spinner("üíæ Storing data in database..."):
    result = store_pv_data(normalized, user_id, organization, source)
    # Blocks UI for 2-5 minutes

# Optimized: Async background task
def store_pv_data_async(df, user_id, organization, source):
    # Queue job, return immediately
    # Process in background thread
    # Update status via WebSocket or polling
```

**Benefits:**
- ‚úÖ UI doesn't block during database storage
- ‚úÖ User can continue using app
- ‚ö†Ô∏è Still server-side, but non-blocking

**Implementation Effort:** Medium (2-3 days)

---

### **Solution 2: Chunked File Processing (Medium-term)**
**Current Code:** `src/app_helpers.py` - `load_all_files()`

**Changes:**
```python
# Current: Load entire file into memory
df = pd.read_csv(file)  # Loads 500K rows at once

# Optimized: Process in chunks
chunk_size = 10000
chunks = []
for chunk in pd.read_csv(file, chunksize=chunk_size):
    # Process chunk
    normalized_chunk = normalize(chunk)
    # Store chunk to database immediately
    store_pv_data(normalized_chunk, user_id, org, source)
    chunks.append(chunk)

# Combine chunks for session state (smaller)
df = pd.concat(chunks)
```

**Benefits:**
- ‚úÖ Lower peak memory usage
- ‚úÖ Database writes happen incrementally
- ‚úÖ Progress tracking per chunk
- ‚úÖ Can handle larger files

**Implementation Effort:** Medium (3-5 days)

---

### **Solution 3: Background Job Queue (Long-term)**
**Architecture Change Required**

**New Components:**
- Background job queue (Celery or RQ)
- Redis for job storage
- Worker processes for processing
- Status tracking (Supabase table)

**Flow:**
```
User uploads file
    ‚Üì
Job queued (immediate response)
    ‚Üì
Worker picks up job
    ‚Üì
Process file in background
    ‚Üì
Store to database
    ‚Üì
Update status (ready/complete)
    ‚Üì
User polls or gets notification
```

**Benefits:**
- ‚úÖ True concurrency (multiple workers)
- ‚úÖ User doesn't wait
- ‚úÖ Better resource utilization
- ‚úÖ Handles many concurrent users

**Implementation Effort:** High (2-3 weeks)

**Additional Infrastructure:**
- Redis server
- Worker processes
- Job monitoring

---

### **Solution 4: Streamlit Cloud Scaling (Deployment)**
**Streamlit Cloud Configuration**

**Options:**
1. **Horizontal Scaling (Multiple Instances)**
   - Streamlit Cloud doesn't natively support this
   - Need load balancer + multiple deployments
   - Complex configuration

2. **Vertical Scaling (Larger Instance)**
   - Upgrade Streamlit Cloud plan
   - More RAM/CPU available
   - Still single process, but more resources
   - Cost: Higher monthly fees

**Limitation:**
- Streamlit Cloud is designed for low-to-medium traffic
- Not ideal for high-concurrency scenarios
- Better for: <50 concurrent users

---

## üèóÔ∏è Architectural Solutions (Major Changes)

### **Option A: Migrate to Dedicated Server**

**Architecture:**
```
Multiple Users
    ‚Üì
Load Balancer (Nginx/AWS ELB)
    ‚Üì
Multiple Streamlit Instances (Horizontal Scaling)
    ‚Üì
Shared Database (Supabase)
    ‚Üì
Background Workers (Celery + Redis)
```

**Components:**
- AWS/GCP/Azure server
- Docker containers for Streamlit
- Load balancer
- Redis for job queue
- Celery workers
- Supabase database (can stay)

**Benefits:**
- ‚úÖ True horizontal scaling
- ‚úÖ Handles 100+ concurrent users
- ‚úÖ Better resource utilization
- ‚úÖ Production-grade

**Cost:** $200-500/month (depending on traffic)

**Implementation Effort:** 4-6 weeks

---

### **Option B: Hybrid Architecture (Recommended)**

**Keep Streamlit + Add Background Processing**

**Architecture:**
```
User Browser
    ‚Üì
Streamlit App (UI only)
    ‚Üì
API Server (FastAPI)
    ‚îú‚îÄ Background Jobs (Celery)
    ‚îú‚îÄ Job Status API
    ‚îî‚îÄ File Upload API
    ‚Üì
Workers (Process files)
    ‚Üì
Supabase Database
```

**Flow:**
1. User uploads file via Streamlit
2. Streamlit calls FastAPI endpoint
3. FastAPI queues background job
4. Streamlit shows "processing..." status
5. Background worker processes file
6. Streamlit polls status API
7. When complete, data available

**Benefits:**
- ‚úÖ Keep Streamlit UI (familiar)
- ‚úÖ Add scalable backend
- ‚úÖ Non-blocking uploads
- ‚úÖ Better concurrency

**Implementation Effort:** 3-4 weeks

---

### **Option C: Client-Side Processing (ChatGPT's Proposal)**

**See:** `ARCHITECTURE_SHIFT_ANALYSIS.md`

**Summary:**
- Move processing to browser (DuckDB WASM)
- Server only for LLM + Auth
- Better for: <100MB files
- Limitation: Browser memory constraints

**Not Recommended:** See detailed analysis in `ARCHITECTURE_SHIFT_ANALYSIS.md`

---

## üìä Scalability Comparison

| Solution | Concurrent Users | Implementation | Cost | Complexity |
|----------|-----------------|----------------|------|------------|
| **Current** | 5-10 | ‚úÖ Done | Low | Low |
| **Optimized DB** | 10-15 | üü° 3 days | Low | Low |
| **Chunked Processing** | 15-25 | üü° 5 days | Low | Medium |
| **Job Queue** | 25-50 | üî¥ 3 weeks | Medium | High |
| **Hybrid (FastAPI)** | 50-100+ | üî¥ 4 weeks | Medium | High |
| **Dedicated Server** | 100+ | üî¥ 6 weeks | High | Very High |
| **Client-Side** | 20-30 | üî¥ 4-6 months | Low | Very High |

---

## üéØ Recommended Approach

### **Phase 1: Immediate (Week 1)**
1. ‚úÖ **Optimize Database Storage**
   - Make database writes non-blocking
   - Return immediately, process in background
   - Update status via polling

2. ‚úÖ **Add File Size Limits**
   - Warn users about large files
   - Suggest chunked uploads
   - Set reasonable limits (e.g., 200MB)

3. ‚úÖ **Add Progress Tracking**
   - Show upload progress
   - Show processing status
   - Show queue position (if multiple users)

### **Phase 2: Short-term (Weeks 2-4)**
1. ‚úÖ **Implement Chunked Processing**
   - Process files in 10K row chunks
   - Store chunks incrementally
   - Lower memory footprint

2. ‚úÖ **Add Background Jobs**
   - Simple queue system
   - Process uploads asynchronously
   - Better user experience

### **Phase 3: Long-term (Months 2-3)**
1. ‚úÖ **Migrate to Hybrid Architecture**
   - FastAPI backend for heavy operations
   - Keep Streamlit for UI
   - True scalability

---

## üîç Current Limitations Summary

| Aspect | Current | Max Capacity | Bottleneck |
|--------|---------|--------------|------------|
| **Concurrent Users** | 5-10 | 10-15 | Memory + CPU |
| **File Size** | 100MB | 200MB | Memory |
| **Total Rows** | 500K | 1M | Memory |
| **Database Inserts** | Blocking | ~1K/sec | Synchronous |
| **Processing Speed** | Sequential | Sequential | Single-threaded |

---

## ‚ö†Ô∏è Critical Issues for Multi-User

### **1. Session State Isolation**
**Current:** ‚úÖ Each user has isolated `st.session_state`
**Problem:** ‚ö†Ô∏è But all share same server memory
**Impact:** If User 1 loads 500MB, less memory for User 2

### **2. Database Connection Pooling**
**Current:** ‚ö†Ô∏è No explicit connection pooling
**Problem:** Each request may create new connection
**Impact:** Connection exhaustion with many users

### **3. File Upload Timeout**
**Current:** ‚ö†Ô∏è No timeout handling
**Problem:** Large files may timeout (30s-2min typical)
**Impact:** Upload fails silently

### **4. Error Recovery**
**Current:** ‚ö†Ô∏è No retry mechanism
**Problem:** Transient failures cause upload loss
**Impact:** User must re-upload

---

## ‚úÖ What Works Well Currently

### **1. Multi-Tenant Isolation**
- ‚úÖ RLS policies work perfectly
- ‚úÖ Users only see their organization's data
- ‚úÖ No cross-tenant data leakage

### **2. Authentication**
- ‚úÖ Supabase Auth handles concurrent logins
- ‚úÖ Session management works
- ‚úÖ No conflicts between users

### **3. Data Persistence**
- ‚úÖ Database storage works for all users
- ‚úÖ RLS ensures proper isolation
- ‚úÖ Data persists across sessions

### **4. Query Processing**
- ‚úÖ Each user's queries are independent
- ‚úÖ Database RLS filters automatically
- ‚úÖ No performance impact between users

---

## üöÄ Immediate Action Plan

### **For Low-Medium Usage (<20 users, <50MB files):**
‚úÖ **Keep current architecture**
- Add file size warnings
- Optimize database storage (async)
- Add progress indicators
- Monitor memory usage

### **For High Usage (>20 users, >100MB files):**
üî¥ **Implement Phase 2-3 solutions**
- Background job queue
- Chunked processing
- Consider hybrid architecture
- Monitor and scale

---

## üìù Code Changes Needed

### **Immediate (Low Effort, High Impact):**

**1. Async Database Storage** (`src/pv_storage.py`)
```python
import threading

def store_pv_data_async(df, user_id, organization, source):
    # Queue job, return immediately
    thread = threading.Thread(
        target=store_pv_data,
        args=(df, user_id, organization, source)
    )
    thread.start()
    return {"status": "queued", "message": "Processing in background"}
```

**2. File Size Limit** (`src/ui/upload_section.py`)
```python
MAX_FILE_SIZE = 200 * 1024 * 1024  # 200MB

if uploaded_file.size > MAX_FILE_SIZE:
    st.error(f"‚ö†Ô∏è File too large ({uploaded_file.size / 1024 / 1024:.1f}MB). Maximum size: 200MB")
    return
```

**3. Progress Tracking** (Add status table to Supabase)
```sql
CREATE TABLE upload_jobs (
    id UUID PRIMARY KEY,
    user_id UUID,
    status TEXT,  -- 'queued', 'processing', 'complete', 'failed'
    progress INTEGER,  -- 0-100
    created_at TIMESTAMP
);
```

---

## üéØ Conclusion

### **Current Architecture Can Handle:**
- ‚úÖ **5-10 concurrent users** (with current setup)
- ‚úÖ **50-100MB files** (with current setup)
- ‚úÖ **500K-1M rows per file** (with current setup)

### **Current Architecture Cannot Handle Well:**
- ‚ùå **>20 concurrent users** (needs scaling)
- ‚ùå **>200MB files** (memory constraints)
- ‚ùå **>2M rows per file** (processing time)

### **Recommended Path:**
1. **Short-term:** Optimize database storage + add file size limits
2. **Medium-term:** Implement background job queue
3. **Long-term:** Migrate to hybrid architecture if needed

### **Key Takeaway:**
Current architecture is **fine for MVP and early users**. Scale as needed based on actual usage patterns, not hypothetical scenarios.

---

**Document Version:** 1.0  
**Last Updated:** November 2025  
**Status:** Analysis complete, recommendations provided

